#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#  K E N D A P Y . F O F E N S _ E V A L
#  evaluate an ensembles of fof files generated by a KENDA experiment
#
#  2018.2 L.Scheck

from __future__ import absolute_import, division, print_function
import numpy as np
import os, sys, getpass, subprocess, argparse, time, re, gc, hashlib
import pickle
from kendapy.ekf import Ekf, tables
from kendapy.experiment import Experiment
from kendapy.time14 import Time14

def fofens_eval( xps, fcst_start_times, veri_times_min, common_obs=True, lfcst=True, **kw ) :
    """
    Compute errors and other statistics for several experiments (optionally) using a common set of observations
    defined by the EKF files for all specified absolute forecast start times and relative verification times
    
    :param xps:              List of Experiment objects or log files 
    :param fcst_start_times: List of forecast start times
    :param veri_times_min:   List of verification times in minutes (0=forecast start time)
    :param common_obs:       If True, use common set of observations for all experiments
    :param lfcst:            If True, evaluate long forecasts instead of cycling forecasts
    :param kw:               Additional keywords to be forwarded to fofens_eval_cycle
    :return:                 Dictionary of the form results[expid][fcst_start_time][veri_time][obstype][varname][...]
    """

    allres = {}
    for fcst_start_time in fcst_start_times :

        # convert relative in absolute verification times
        veri_times = [(Time14(fcst_start_time) + Time14(t*60)).string14() for t in veri_times_min]
        #print 'forecast start time ', fcst_start_time, ' : verification times = ', veri_times
        for veri_time in veri_times :
            print('>>>> FCST ', fcst_start_time, ' >>>> VERI ', veri_time)

            if common_obs : # use a common set of observations for all experiments
                res = fofens_eval_cycle( xps, fcst_start_time, veri_time, **kw )
                for expid in res :
                    if expid == 'input' : continue
                    if not expid           in allres        : allres[expid] = {}
                    if not fcst_start_time in allres[expid] : allres[expid][fcst_start_time] = {}
                    allres[expid][fcst_start_time][veri_time] = res[expid]

            else :  # use potentially different sets of observations for the different experiments
                for l in range(len(xps)) :
                    expid = xps[l].expid
                    res = fofens_eval_cycle( [xps[l]], fcst_start_time, veri_time, **kw )
                    if not expid           in allres        : allres[expid] = {}
                    if not fcst_start_time in allres[expid] : allres[expid][fcst_start_time] = {}
                    allres[expid][fcst_start_time][veri_time] = res[expid]

    return allres

#-----------------------------------------------------------------------------------------------------------------------
def otvn_combinations(fee) :

    # find all obstype / varname combinations and all veri_times
    otvn = {}
    vt   = set()
    fst  = set()
    for expid in fee :
        for fcst_start_time in fee[expid] :
            fst.add(fcst_start_time)
            for veri_time in fee[expid][fcst_start_time] :
                vt.add( int((Time14(veri_time).epoch_sec()-Time14(fcst_start_time).epoch_sec())//60) )
                for obstype in fee[expid][fcst_start_time][veri_time] :
                    if not obstype in otvn :
                        otvn[obstype] = set()
                    for varname in fee[expid][fcst_start_time][veri_time][obstype] :
                        otvn[obstype].add(varname)
    return otvn, sorted(list(fst)), sorted(list(vt))

#-----------------------------------------------------------------------------------------------------------------------
def fofens_average(fee) :
    """
    Average result of fofens_eval over all fcst start times
    :param fee: result from fofenc_eval ( fee[expid][fcst_start_time][veri_time][obstype]['overall'][...] )
    :return:    like fee, but without the fcst_start_time dimension
    """
    otvn, fcst_start_times, veri_times_min = otvn_combinations(fee)

    afee = {}
    for expid in fee :
        afee[expid] = {}

        for fcst_start_time in fcst_start_times :

            for veri_time_min in veri_times_min :
                if not veri_time_min in afee[expid] :
                    afee[expid][veri_time_min] = {}
                veri_time = (Time14(fcst_start_time) + Time14(veri_time_min*60)).string14()

                for obstype in fee[expid][fcst_start_time][veri_time] :
                    if not obstype in afee[expid][veri_time_min] :
                        afee[expid][veri_time_min][obstype] = {}

                    for varname in fee[expid][fcst_start_time][veri_time][obstype] :
                        if not varname in afee[expid][veri_time_min][obstype] :
                            afee[expid][veri_time_min][obstype][varname] = {'n_obs':0}

                        # accumulate number of observations (also for veri_time_min == 0, where we have only EKF)
                        n_obs = fee[expid][fcst_start_time][veri_time][obstype][varname]['n_obs']
                        afee[expid][veri_time_min][obstype][varname]['n_obs'] += n_obs

                        # accumulate 'overall' fof rmse/bias
                        if 'overall' in fee[expid][fcst_start_time][veri_time][obstype][varname] :
                            if not 'overall' in afee[expid][veri_time_min][obstype][varname] :
                                afee[expid][veri_time_min][obstype][varname]['overall'] = {'rmse':0, 'bias':0, 'n_obs':0}

                            f = fee[expid][fcst_start_time][veri_time][obstype][varname]['overall']
                            # accumulate number of observations from FOF files
                            afee[expid][veri_time_min][obstype][varname]['overall']['n_obs'] += n_obs
                            afee[expid][veri_time_min][obstype][varname]['overall']['rmse'] += n_obs * f['rmse']**2
                            afee[expid][veri_time_min][obstype][varname]['overall']['bias'] += n_obs * f['bias']

        # correct accumulated rmse and bias using total number of observations
        for veri_time_min in veri_times_min :
            for obstype in afee[expid][veri_time_min] :
                for varname in afee[expid][veri_time_min][obstype] :
                    a = afee[expid][veri_time_min][obstype][varname]
                    if 'overall' in a :
                        n_obs = a['overall']['n_obs']
                        a['overall']['rmse'] = np.sqrt( a['overall']['rmse'] / n_obs )
                        a['overall']['bias'] /= n_obs
    return afee

#-----------------------------------------------------------------------------------------------------------------------
def fofens_eval_cycle( xps, fcst_start_time, veri_time, lfcst=True, obstypes=None, varnames=None, verbose=True,
                       cache=True, recompute=False, **kw ) :
    """
    Compute errors and other statistics from FOF files for several experiments using a common set of observations
    defined by the EKF files for a certain analysis cycle
    
    :param xps:             List of Experiment objects or log files 
    :param fcst_start_time: Forecast start time
    :param veri_time:       Analysis time for which EKF files will be used
    :param lfcst:           If True, read long forecast FOFs
    :return:                result dictionary of the form results[expid][obstype][varname][...]
    """

    if obstypes is None : obstypes = list(tables['obstypes'].values())
    if varnames is None : varnames = list(tables['varnames'].values())

    n_xps  = len(xps)

    # construct cache file name (such that the order of the experiments does not matter)
    expids = sorted([ xps[l].expid for l in range(n_xps) ])
    identifier = (','.join(expids)) + (','.join(obstypes)) + (','.join(varnames)) \
               + ('lfcst' if lfcst else 'cycle') + fcst_start_time + veri_time
    if len(kw) > 0 :
        if len(kw) > 1 or (not 'state_filter' in kw) or kw['state_filter'] != 'active' :
            for k in kw :
                identifier += '_'+k+':'+kw[k]
    print('fofens_eval_cycle: using identifier %s ...' % identifier)
    # use md5 hash instead of identifier to avoid too long file names
    hash_object = hashlib.md5(identifier.encode())
    if len(xps) == 1 :
        cache_dir = xps[0].settings['PLOT_DIR']+'/cache/fofens_eval'
    else :
        cache_dir = xps[0].settings['PLOT_DIR'].replace(xps[0].expid,'') +'/cache/fofens_eval'
    cache_fname = cache_dir + '/fofens_eval_'+hash_object.hexdigest()

    if cache and os.path.exists(cache_fname) and not recompute : # use cached results ..................................
        if verbose : print('[fofens_eval_cycle] reading cache file %s ...' % cache_fname)
        with open(cache_fname, 'r') as f :
            results = pickle.load(f)

    else : # compute results ...........................................................................................
        if verbose : print('[fofens_eval_cycle] computing results that will be stored in ', cache_fname)
        results = { 'input':{ 'fcst_start_time':fcst_start_time,
                              'veri_time':veri_time,
                              'lfcst':lfcst,
                              'obstypes':obstypes,
                              'varnames':varnames,
                              'expids':expids }}

        ekf_time_shift = (Time14(veri_time) - Time14(fcst_start_time)).daymin()
        if verbose : print('[fofens_eval_cycle] reference time difference : %d minutes ' % ekf_time_shift)

        if verbose : print('[fofens_eval_cycle] reading EKFs... ')
        ekfs = {}
        obstypes_avail = set()
        for l in range(n_xps) :
            ekfs[ xps[l].expid ] = {}
            if verbose : print('                   ', l, xps[l].expid, end=' ')
            for obstype in obstypes :
                if os.path.exists( xps[l].ekf_filename( veri_time, obstype ) ) :
                    if verbose : print(obstype, end=' ')
                    ekfs[ xps[l].expid ][obstype] = xps[l].get_ekf( veri_time, obstype, cache=False, **kw )
                    obstypes_avail.add(obstype)
            if verbose : print()
        obstypes_avail = list(obstypes_avail)
        if verbose : print('[fofens_eval_cycle] available observation types : ', obstypes_avail)

        if verbose : print('[fofens_eval_cycle] determining common EKF observation subset...')
        default_filter = ekfs[ xps[0].expid ][obstypes_avail[0]].filter
        if verbose : print('                    default filter : ', default_filter)
        ekfids = {}
        for obstype in sorted(obstypes_avail) :
            ekfids[obstype] = {}
            varnames_ = sorted(ekfs[ xps[0].expid ][obstype].varnames)
            for varname in varnames_ :
                if not varname in varnames :
                    continue
                if verbose : print('                   %10s / %10s : ' % ( obstype, varname ), end=' ')
                oids = None
                for l in range(n_xps) :
                    if obstype in ekfs[ xps[l].expid ] :
                        # consider only observations with the correct variable name
                        ekfs[ xps[l].expid ][obstype].replace_filter( filter=default_filter, varname_filter=varname )

                        if oids is None :
                            # compute unique identifier for each observation
                            oids, nonunique = ekfs[ xps[l].expid ][obstype].identify( time_shift=ekf_time_shift,
                                                                                      filtername='var_'+varname, )
                            if verbose : print('%5d (%d non-unique) ' % (len(oids),len(nonunique)), end=' ')

                        else :
                            # discard all observation that are not present in all of the experiments
                            oids, unknown   = ekfs[ xps[l].expid ][obstype].identify( time_shift=ekf_time_shift,
                                                                                      filtername='var_'+varname,
                                                                                      allowed=oids )
                            if oids is None :
                                print('NO OBSERVATIONS SURVIVED! ', obstype, varname, xps[l].expid)
                            if verbose : print('%5d ' % len(oids), end=' ')
                if verbose : print()
                if not oids is None :
                    ekfids[obstype][varname] = oids

        if ekf_time_shift > 0 : # for fcst_start_time == veri_time there are no FOFs, but EKFs
            if verbose : print('[fofens_eval_cycle] reading FOFs... ', end=' ')
            fofens = {}
            for l in range(n_xps) :
                if verbose : print(xps[l].expid, end=' ')
                # create a list of Ekf objects for all member FOF files
                fofens[ xps[l].expid ] = [ xps[l].get_fof( fcst_start_time, memidx=m, lfcst=lfcst ) for m in range(1,xps[l].n_ens+1) ]
            if verbose : print()

        if verbose : print('[fofens_eval_cycle] assembling and evaluating ensemble data... ')
        for obstype in sorted(ekfids.keys()) :
            for varname in sorted(ekfids[obstype].keys()) :

                # get EKF observations ids
                oids = ekfids[obstype][varname]
                filtername = 'common_%s_%s'%(obstype,varname)
                if verbose : print('                   %10s / %10s : %5d common EKF observations' % ( obstype, varname, len(oids) ), end=' ')

                if ekf_time_shift > 0 : # for fcst_start_time == veri_time there are no FOFs, but EKFs
                    # identify the common EKF observations in each of the FOF files
                    for l in range(n_xps) :
                        #print 'initializing unknown set', l
                        unknown = set()
                        for i in range(xps[l].n_ens) :
                            # consider only observations with the correct obstype and varname, but with any state
                            fofens[ xps[l].expid ][i].replace_filter( filter=default_filter, obstype_filter=obstype, varname_filter=varname, state_filter='' )
                            #print 'filter name : ', fofens[ xps[l].expid ][i].filtername

                            # all common EKF observations should be found in all FOF files...
                            oids, unknown_i = fofens[ xps[l].expid ][i].identify( allowed=oids, filtername=filtername)
                            #print 'unknwon_i', i, unknown_i
                            if not unknown_i is None :
                                unknown.update(unknown_i)
                            #print 'found %d of %d EKF observations (%d other observations)...' % (len(oids), len(ekfids[obstype][varname]), len(unknown_i))
                        #if verbose : print '%s : found %d of %d EKF observations (%d other observations)...' % ( xps[l].expid, len(oids), len(ekfids[obstype][varname]), len(unknown))
                        if verbose : print(', %s:%5d' % (xps[l].expid, len(oids)), end=' ')
                    if len(oids) != len(ekfids[obstype][varname]) :
                        print(' <-- WARNING : Lost some observations!', obstype, varname, len(oids), len(ekfids[obstype][varname]))
                if verbose : print()

                # assemble first guess and observation arrays
                for l in range(n_xps) :

                    # output dictionary
                    res = {'n_obs':len(oids)}

                    # compute rmse, bias, spread from EKF files
                    e = ekfs[ xps[l].expid ][obstype]
                    e.replace_filter( filter=default_filter, varname_filter=varname )
                    e.identify( time_shift=ekf_time_shift, filtername='common_'+varname, allowed=oids )
                    eobs    = np.array( [ e.data['obs'                     ][e.oids['common_'+varname][oid]] for oid in oids ] )
                    efgmean = np.array( [ e.data['veri_data'][e.i_fgmean, :][e.oids['common_'+varname][oid]] for oid in oids ] )
                    anamean = np.array( [ e.data['veri_data'][e.i_anamean,:][e.oids['common_'+varname][oid]] for oid in oids ] )
                    res['overall_ekf'] = { \
                        'bias'     : (efgmean - eobs).mean(),
                        'rmse'     : np.sqrt( ((efgmean - eobs)**2).mean() ),
                        'ana_bias' : (anamean - eobs).mean(),
                        'ana_rmse' : np.sqrt( ((anamean - eobs)**2).mean() ) }

                    if ekf_time_shift > 0 : # for fcst_start_time == veri_time there are no FOFs, but EKFs

                        # build ensemble array from FOF data
                        fg = np.zeros(( xps[l].n_ens, len(oids) ))
                        for i in range(xps[l].n_ens) :
                            f = fofens[ xps[l].expid ][i]
                            fg[i,:] = np.array( [ f.data['veri_data'][f.i_fgens_first,:][f.oids[filtername][oid]] for oid in oids ] )
                        fgmean = fg.mean(axis=0)
                        # build observation array
                        obs = np.array( [ f.data['obs'][f.oids[filtername][oid]] for oid in oids ] )

                        level = np.array( [ f.data['level'][f.oids[filtername][oid]] for oid in oids ] )

                        # compute ensemble mean rmse, bias, spread
                        res['overall'] = { \
                            'bias'     : (fgmean - obs).mean(),
                            'rmse'     : np.sqrt( ((fgmean - obs)**2).mean() ),
                            'spread'   : np.sqrt( (fg.std(axis=0,ddof=1)**2).mean() ),
                            'dobsmax'  : abs(obs-eobs).max()  if obs.size > 0 else 0,
                            'dobsmean' : abs(obs-eobs).mean() if obs.size > 0 else 0 }
                            # spread = square root of mean variance

                    if not xps[l].expid in results               : results[xps[l].expid] = {}
                    if not obstype      in results[xps[l].expid] : results[xps[l].expid][obstype] = {}
                    results[xps[l].expid][obstype][varname] = res

        # save results to cache file
        if cache :
            if verbose : print('[fofens_eval_cycle] writing results into ', cache_fname)
            if not os.path.exists(cache_dir) :
                os.makedirs(cache_dir)
            with open(cache_fname, 'w') as f :
                pickle.dump( results, f, pickle.HIGHEST_PROTOCOL )

    # end of 'use cached results' ......................................................................................

    return results

#-------------------------------------------------------------------------------
if __name__ == "__main__": # ---------------------------------------------------
#-------------------------------------------------------------------------------

    parser = argparse.ArgumentParser(description='Evaluate FOF file ensembles')

    parser.add_argument( '-s', '--start-time',    dest='start_time', help='(first) fcst start time',    default='' )
    parser.add_argument( '-e', '--end-time',      dest='end_time',   help='last    fcst start time',    default='' )
    parser.add_argument( '-a', '--analysis-time', dest='veri_time',  help='analysis time',      default='' )
    parser.add_argument( '-c', '--cycle',         dest='cycle',      help='use cycle fg instead of long forecasts',   action='store_true' )
    parser.add_argument(       '--common',        dest='common',     help='use common set of observations for all experiments',   action='store_true' )

    parser.add_argument(       '--recompute',   dest='recompute',  help='do not read cache files', action='store_true' )

    parser.add_argument(       '--colors',      dest='colors',     help='comma-separated list of colors for the experiments', default='' )
    parser.add_argument(       '--names',       dest='names',      help='comma-separated list of names (use ,, for commas within the names)', default='' )

    parser.add_argument( '-A', '--area-filter',  dest='area_filter',  help='area filter for observations', default='auto' )
    parser.add_argument( '-S', '--state-filter', dest='state_filter', help='state filter for observations', default='active' )
    parser.add_argument( '-V', '--variables',    dest='varnames',    help='comma-separated list of variables  to be considered (default: all)', default='' )
    parser.add_argument( '-O', '--obstypes',     dest='obstypes',    help='comma-separated list of obs. types to be considered (default: all)', default='' )

    parser.add_argument(       '--dwd2lmu',     dest='dwd2lmu',     help='convert settings',  action='store_true' )

    parser.add_argument( '-p', '--path',        dest='output_path', help='path to the directory in which the plots will be saved', default='' )
    parser.add_argument( '-I', '--input-path',  dest='input_path',  help='path to the directory containing the log files', default='' )
    parser.add_argument( '-f', '--filetype',    dest='file_type',   help='file type [ png (default) | pdf | eps | svg ...]', default='png' )
    parser.add_argument( '-v', '--verbose',     dest='verbose',     help='be more verbose',   action='store_true' )

    parser.add_argument( 'logfile', metavar='logfile', help='log file name', nargs='*' )
    args = parser.parse_args()

    logfiles = args.logfile
    for i, lfn in enumerate(logfiles) :
        if not lfn.endswith('.log') : # asssume it is an experiment id and not a log file
            logfiles[i] += '/run_cycle_'+logfiles[i]+'.log'
        if args.input_path != '' : # add input path
            logfiles[i] = os.path.join( args.input_path, logfiles[i] )

    xps = []
    for i, logfile in enumerate(logfiles) :
        xp = Experiment(logfile)
        print('experiment %s : %s' % ( xp.settings['exp'], xp.description() ))
        xps.append( xp )

    if args.end_time != '' : # process all forecasts between start_time and end_time and all veri_times

        fcst_start_times = [f for f in (xps[0].fcst_start_times if args.cycle else xps[0].lfcst_start_times) if (Time14(f) >= Time14(args.start_time)) & (Time14(f) <= Time14(args.end_time))]
        assint_min = int(xps[0].settings['ASSINT'])//60
        if args.cycle :
            veri_times_min = [assint_min]
        else :
            #veri_times_min = range( assint_min, (int(xps[0].lfcst_settings['FCTIME'])//60)+1, assint_min )
            veri_times_min = list(range(          0, (int(xps[0].lfcst_settings['FCTIME'])//60)+1, assint_min))
        #print fcst_start_times, veri_times_min

        res = fofens_eval( xps, fcst_start_times, veri_times_min, lfcst=(not args.cycle), common_obs=args.common,
                           obstypes = args.obstypes.split(',') if args.obstypes != '' else None,
                           varnames = args.varnames.split(',') if args.varnames != '' else None,
                           recompute=args.recompute )

        print('experiments ', list(res.keys()))
        for expid in res :
            for fcst_start_time in res[expid] :
                print('EXPID %10s, fcst %s :' % (expid, fcst_start_time))
                for veri_time in res[expid][fcst_start_time] :
                    print(veri_time, res[expid][fcst_start_time][veri_time]['AIREP']['RH']['overall']['rmse'], '|', end=' ')
                print()

        #results[expid][fcst_start_time][veri_time][obstype][varname][...]

    else : # process only one start_time / veri_time case

        res = fofens_eval_cycle( xps, args.start_time, args.veri_time, lfcst=(not args.cycle),
                                 obstypes = args.obstypes.split(',') if args.obstypes != '' else None,
                                 varnames = args.varnames.split(',') if args.varnames != '' else None,
                                 recompute=args.recompute )
        print(res)



